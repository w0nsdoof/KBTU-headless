# Project Context: KBTU Headless CMS Migration

## Overview

This directory contains the work-in-progress for migrating the KBTU website (kbtu.edu.kz) from its current monolithic structure to a modern, headless CMS architecture. The project is structured into three main components:

1.  **`playwright-mcp`**: A Playwright-based crawler designed to extract structured data from the KBTU English website (`/en/`), focusing on key entities like Faculties, Departments, Programs, Pages, and Topics. It outputs normalized JSONL files for backend consumption.
2.  **`python-crawler`**: A secondary, broader Python-based crawler using `aiohttp` for a general audit of the English site, capturing URLs, SEO data, content samples, media, forms, and integrations. It outputs CSV and JSONL files for analysis.
3.  **`summary-python-crawler`**: A tool that analyzes the data from `python-crawler` to produce summaries for PRD and headless migration planning. It focuses on identifying content patterns, types, and SEO considerations.

## Key Directories & Files

### `playwright-mcp/`

This is the primary crawler for structured data extraction, intended for direct backend import.

*   **`QWEN.md`**: Defines the crawling mission, scope (entities), policies, data extraction format, and output structure for the Playwright crawler. It targets a Django/DRF backend for the headless CMS.
*   **`PRD.md`**: Contains the draft Product Requirements Document (PRD) for the backend of the headless CMS. It details functional requirements, user roles, data models, APIs, and a release plan. This document is the basis for backend development.
*   **`backend.md`**: Provides a concise summary of the planned backend entities (Faculty, Department, Program, Page, Topic, MediaAsset), their relationships (ERD), modeling principles, and Django app structure. It's a technical companion to `PRD.md`.
*   **`prompt.md`**: (Likely contains the specific prompt used for the Playwright-MCP agent).
*   **`out/`**: Contains the output of the Playwright crawl.
    *   `faculties.jsonl`, `departments.jsonl`, etc.: Normalized JSONL files for each entity type.
    *   `_report.json`: A summary report of the crawl (e.g., 8 faculties, 12 departments extracted).
*   **`state/`**: (Likely contains crawler state snapshots).

### `python-crawler/`

This is a general-purpose website auditor.

*   **`QWEN.md`**: Defines the scope, crawling mechanics, and data extraction targets (URLs, SEO, content, media, forms, etc.) for the Python crawler. It's broader than `playwright-mcp`.
*   **`crawler.py`**: The main Python script implementing the asynchronous crawl using `aiohttp`. It handles URL normalization, filtering, rate-limiting, and saving output files.
*   **`.qwen/`**: (Internal directory for tool state).

### `summary-python-crawler/`

This is an analysis tool for the data generated by `python-crawler`.

*   **`QWEN.md`**: Defines the role of this component as an expert in content architecture and API design, tasked with analyzing crawled datasets to produce summaries for PRD and migration planning.
*   **`data/`**: (Likely the input directory for raw crawl data from `python-crawler`).
*   **`output/`**: (Likely the output directory for generated summaries).
*   **`.qwen/`**: (Internal directory for tool state).

## Workflow

1.  The `python-crawler` performs a broad audit of the KBTU English site.
2.  The `summary-python-crawler` analyzes the audit data to understand site structure and content.
3.  The `playwright-mcp` performs a targeted crawl to extract structured data for specific entities defined in its `QWEN.md` and the `PRD.md`.
4.  The output from `playwright-mcp` (`out/*.jsonl`) is intended to be the primary data source for importing into the new Django/DRF backend, as outlined in `PRD.md` and `backend.md`.

## Key Technologies & Concepts

*   **Headless CMS**: Decoupling the backend content management from the frontend presentation layer.
*   **Django/DRF**: The chosen backend framework for the new CMS.
*   **Angular**: The planned frontend framework (consumer of the headless API).
*   **Structured Data Extraction**: Extracting content into well-defined entities (Faculty, Program, etc.) rather than raw HTML.
*   **JSONL**: A data format used for importing extracted entities (`playwright-mcp/out/*.jsonl`).
*   **RBAC**: Role-Based Access Control planned for content editing.
*   **Multilingual Support**: Storing translations for content entities.